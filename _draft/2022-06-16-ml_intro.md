---
layout: post
title: 'ML'
subtitle: 'ML intro'
categories: dlog
tags: ml
comments: true
---

# ML intro

## Cost Function with Loss function and Regrularization
## Gradient Descent

## BinaryCrossEntropy
## Softmax

## Backpropagation

## Precision Recall trandoff
## Bias Variance
    - 정의, 디그리와 관계(train,dev error), 람다와관계(train,dev error)
    - 트레이닝셋사이즈와 관계



1 Supervised Learning
input output
email, spam, spam filtering
audio, text transcripts speech recognition
engilish korean machine translation
ad userinfo, click online advertising
image radarinfo, position of other cars self-driving car
image of phone, defect visual inspection

1-1 Regression
input, output
house size, price
> predict number

|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |
|: ------------|: ------------------------------------------------------------||
| $a$ | scalar, non bold                                                      ||
| $\mathbf{a}$ | vector, bold                                                      ||
| **Regression** |         |    |     |
|  $\mathbf{x}$ | Training Example feature values (in this lab - Size (1000 sqft))  | `x_train` |   
|  $\mathbf{y}$  | Training Example  targets (in this lab Price (1000s of dollars))  | `y_train` 
|  $x^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `x_i`, `y_i`|
| m | Number of training examples | `m`|
|  $w$  |  parameter: weight                                 | `w`    |
|  $b$           |  parameter: bias                                           | `b`    |     
| $f_{w,b}(x^{(i)})$ | The result of the model evaluation at $x^{(i)}$ parameterized by $w,b$: $f_{w,b}(x^{(i)}) = wx^{(i)}+b$  | `f_wb` | 


x = "input" variable, feature
Training set = data used to train in the model
y = "output" variable, "target" variable
m=number of training examples
(x,y) = single training example
x^(i)(!= square), y^(i) = i^th traiing example

training set x=features
-> f = function = hypothesis
-> y-hat=estimated y(targets)

f_w,b(x) = wx+b
"linear regression with one variable = univariate linear regression
w, b are "fit" using training data.
b ="y-intercept"
w="weights"
w,b are parameter of model = coefficient = weights

---
Cost function
squared error cost function
J(w,b) = 1/m*[sum_i=1~m:number of training examples { (y-hat-y:error)^2}]
IMPORTANT!!!!!!!!!
how to find J function? and how to minimize?
IMPORTANT!!!!!!!!!
how - gradient descent
중요한점 - 모든 파라메터를(w,b) 동시에업데이트해야한다. 하나씩하면 효과는 있지만, 옳지않은 방법. w업데이트후 b미분해서업데이트하면 안됨.
로컬미니멈빠지면 미분값0이라 더이상 가중치업데이트 없음.
최소값가까워질수록 경사도가 완만하여 업데이트가 완만히 되고 수렴하게됨.

1주차강의에서 제일중요한 점은 비용함수

* 2주차
multi variable(features) regression
(not multivariate regression, multiple regression O)
-앵간하면 벡터화병렬계산하자 (for문말고)
-normalization 등고선그리면 스케일큰변수의 파라메터가 등고선휙휙변하게함
mean-norm, zscore-norm
feature engineering(**2) 등을 하면 후에 꼭 스케일링할것

??
d/dw sum(wx+b-y)**2
=sum(wx+b-y)* 2*x

1-2 Classification
input(1 or more), output
tumor size, breast cancer detection (diagnosis)
> predict categories


two class = binary classification
loss function = 단일 샘플에서 예측값과 실제값의 차이를 손실함수
cost function = 전체샘플에서 loss function합한것 (sum(L(f,y)))
여기서 loss funciton은 squared error 처럼 convex해야하는것이며 이증명은 수학과로 넘김 (그래야만 수렴가능)
binary cross entropy = logistic regression cost function

* 정규화
underfitting
모델이 데이터를 완전히 맞추지 못하고 있을 때 = high bias
편견을가지고 한쪽으로만(?) 예측하는녀석이다
overfitting
high variance
2차곡선으로맞춫걸 4차곡선으로맞추니 x값에따라 y값의 분산이 높은거다!!
-해결책
    1 훈련데이터추가
    2 Feature selection
        행에비해 상대적으로 적은 열
    3 Regularization
        w1~wn 파라메터를 선택적으로 크기를 줄임(중요한건크게, 안중요한건 작게0.0001)
        관습적으로 b는 주로 추가하지않음
        그리고 cost때와 마찬가지로 관습적으로 2m(훈련셋크기*2)로 나눔 미분할때편하게하려고인듯..?!
        cost=sum(Loss!!:MSE)+Sum(Regrularization term)
        -> fit data + keep w small goal both!!!
        람다크면 w많이작아지고(수평선될수도) 작으면(0이면) 그냥 오버피팅그대로두는것



2 Unsupervised Learning
clustering
News, DNA microarray, clustomer grouping

Anomaly detection, 




---
activation
task classification : sigmoid
regression (including positive, negative): linear
regression (only positive) : relu
hidden relu (fast compute)
필요한 이유


SOFTMAX
sparse categorical crossentropy
sparse 하나만 선택가능

multilabel != multiclass

* 역전파의 효율성
gradient descent과정에서 for rows : for col(units) 즉, N(데이터개수)*P(파라미터개수) 이중포문을 돌게되는데 이 과정을 줄여줄수있다!!
10,000데이터에 100,000파라메터 모델이면 기존에는 10^9 , 역전파쓰면 10^5(N+P)

Q d변화량, J변화량, a변화량을 어떻게 구하는지? (0.001 올랐다, a=4.001이다 등)
    -> a=4.001인거는 기존 업데이트된 값을 보는건가,,
    -> 0.001올랐으면 dev(J)/dev(썸띵) = 1인거?
Q dev(J)/dev(a)는 2인게 어떻게 구하는지? -> 체인룰
젤첨에는 dev(J)/dev(d)인데 이는 수식이 있으니 구할수있음
    -> J=1/2*d^2 여기서 dev=2?? 왜..

---

---
평가지표
!!테스트할때는 정규화항 넣지않음!!
train/val=dev=developmentset=crossvalidation/test

* 편향과분산
high bias : 모델변경
high variance : 데이터추가
* 파이프라인
-오류분석
-데이터추가(무작정추가하지않고, 오류분석으로 약한부분에만.예를들어 약관련스팸메일 오탐, 아규멘테이션)
    전통:modelcentric 
    지금:datacentric
-전이학습
    1 헤드만
    2 전체다
-MLops

* unbalance


Git을 알아보자

![title](/img/path/img.png)


## 1. Git 초기 설정

* **blah** blah
    * balh

    ```bash
    bash code
    ```